{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanvelc/NN/blob/main/M6_AST_07_Graph_Neural_Networks_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-QH8csEB8V"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 7: Node Classification with Graph Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTWXpKFKEgWp"
      },
      "source": [
        "## Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSWznr2PEhq_"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* Understand what is Node Classification with Graph Neural Networks\n",
        "* Perform real-time data analytics with Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag6FJg75E8cr"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-mRLQqYE9Vp"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTJU9Dr-FBBP"
      },
      "source": [
        "Many datasets in various machine learning (ML) applications have structural relationships between their entities, which can be represented as graphs. Such application includes social and communication networks analysis, traffic prediction, and fraud detection. Graph representation Learning aims to build and train models for graph datasets to be used for a variety of ML tasks.\n",
        "\n",
        "This example demonstrates a simple implementation of a Graph Neural Network (GNN) model. The model is used for a node prediction task on the Cora dataset to predict the subject of a paper given its words and citations network.\n",
        "\n",
        "Note that, a Graph Convolution Layer is implemented in this notebook from scratch to provide better understanding of how they work. However, there is a number of specialized TensorFlow-based libraries that provide rich GNN APIs, such as Spectral, StellarGraph, and GraphNets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAvCJ0EZN8vU"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The [Cora dataset](https://linqs.org/datasets/#cora) consists of 2,708 scientific papers classified into one of seven classes. The citation network consists of 5,429 links.\n",
        "Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary.\n",
        "For each publication (paper), there is a 0/1-valued word vector which is basically a binary word vector of size 1,433, indicating the absence/presence of a corresponding word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adZ3ge_NFajV"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2aoDFa2FqhA"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbpFp_W-Ft6j"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TsNhG5brn35i"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M6_AST_07_Graph_Neural_Networks_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget https://cdn.exec.talentsprint.com/static/cds/content/cora.zip\")\n",
        "    !unzip /content/cora.zip\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMis00kAoTw0"
      },
      "source": [
        "# Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im3QZY1mF8-N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTqEFrU5GW9W"
      },
      "source": [
        "As imported in the above code cell, the **networkx** library is a Python package used for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
        "This helps to implement an undirected graph, directed graphs, and multigraphs (graphs with multiple edges between nodes).\n",
        "- **networkx** includes a wide range of graph algorithms, such as shortest path, clustering, and network flow algorithms to create graphs, add nodes and edges, and access various properties and attributes of the graph.\n",
        "- **networkx** provides basic drawing capabilities to visualize graphs. Although not as advanced as specialized visualization libraries, it's useful for quick visualizations.\n",
        "- The **networkx** library supports importing and exporting graph data from/to various formats like adjacency lists, edge lists and others\n",
        "\n",
        "To know more about **networkx** library, please visit [NetworkX](https://networkx.org/documentation/stable/reference/introduction.html#networkx-basics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf2swQYRGJCt"
      },
      "source": [
        "# Prepare the Dataset\n",
        "The following tasks are accomplished to prepare the dataset \"cora\" for further processing, visualization and to create the graph structure using NetworkX.\n",
        "Both the files (i.e., cora.cites and cora.content) existing under the unzipped folder 'cora' in the data directory '/content/cora', will be used in the subsequent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4sIVDszGbZY"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/cora'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4CdVfj8GoVs"
      },
      "source": [
        "# Process and visualize the dataset\n",
        "\n",
        "As stated above, the dataset has two tap-separated files: cora.cites and cora.content.\n",
        "\n",
        "The cora.cites includes the citation records with two columns: cited_paper_id (target) and citing_paper_id (source).\n",
        "The cora.content includes the paper content records with 1,435 columns: paper_id, subject, and 1,433 binary features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFTe_wTNljrp"
      },
      "source": [
        "## Load data\n",
        "Load the citations data into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzfImes6Gve5"
      },
      "outputs": [],
      "source": [
        "citations = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.cites\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"target\", \"source\"],\n",
        ")\n",
        "print(\"Citations shape:\", citations.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va1II7DUG7jU"
      },
      "source": [
        "Display a sample of the citations DataFrame. The target column includes the paper ids cited by the paper ids in the source column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S_gUaK_G8oh"
      },
      "outputs": [],
      "source": [
        "citations.sample(frac=1).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFoCCMp0HEm6"
      },
      "source": [
        "Now load the papers data into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLLt3AVlHFcS"
      },
      "outputs": [],
      "source": [
        "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
        "papers = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.content\"), sep=\"\\t\", header=None, names=column_names,\n",
        ")\n",
        "print(\"Papers shape:\", papers.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE0HBUdHUDH"
      },
      "source": [
        "Now display a sample of the papers DataFrame. The DataFrame includes the paper_id and the subject columns, as well as 1,433 binary column representing whether a term exists in the paper or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nar7kYJbHViE"
      },
      "outputs": [],
      "source": [
        "print(papers.sample(5).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LIuOqcaorbe"
      },
      "source": [
        "From the above result, it is observed that,\n",
        "- **paper_id** contains the unique ID for each paper. For example, paper IDs are 9515, 1110563, 219239, 1116629, and 1134056.\n",
        "- **term_0 to term_1432:** It represent a sparse bag-of-words feature vector for the paper. Each term corresponds to a specific word in the corpus, and the binary values indicate whether that word appears (i.e., 1) or doesn't appear (i.e., 0) in the paper.\n",
        "- **subject:** Provides the classification or category of each paper, which can be used as labels for training machine learning models.\n",
        "- For instance in the above result, the paper having **paper_id 9515** does not contain any of the terms from term_0 to term_1432 and belongs to the subject category \"Theory\".\n",
        "- And the paper having **paper_id 1110563** also does not contain any of the terms from term_0 to term_1432 and belongs to the subject category \"Neural_Networks\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A5URVSHHZje"
      },
      "source": [
        "Display the count of the papers in each subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHfA8uXwHdWN"
      },
      "outputs": [],
      "source": [
        "print(papers.subject.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkT3icxcJiE7"
      },
      "source": [
        "The above result of the command **papers.subject.value_counts()** provides the count of papers in each subject within the dataset.\n",
        "1. Neural Networks: This subject has the highest number of papers, with a total of 818 papers. This suggests a strong focus or large amount of research in the field of neural networks.\n",
        "\n",
        "2. Probabilistic Methods: The second most common subject, with 426 papers. This indicates a significant interest and research activity in probabilistic methods.\n",
        "\n",
        "3. Genetic Algorithms: There are 418 papers in this subject, highlighting a substantial amount of work in the area of genetic algorithms.\n",
        "\n",
        "4. Theory: This subject has 351 papers. Theory-related research is also well-represented in the dataset.\n",
        "\n",
        "5. Case-Based: With 298 papers, case-based methods are another important area of research covered in this dataset.\n",
        "\n",
        "6. Reinforcement Learning: There are 217 papers focused on reinforcement learning. This is a smaller but still notable area of research.\n",
        "\n",
        "7. Rule Learning: The subject with the fewest papers, having 180 papers. This suggests relatively less research activity in rule learning compared to the other subjects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWQa1SjjHhK2"
      },
      "source": [
        "Convert the paper ids and the subjects into zero-based indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9PILYEVHkit"
      },
      "outputs": [],
      "source": [
        "class_values = sorted(papers[\"subject\"].unique())\n",
        "class_idx = {name: id for id, name in enumerate(class_values)}\n",
        "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
        "\n",
        "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
        "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIyQrUKiLJr5"
      },
      "source": [
        "**In the above code cell**, it performs the task of converting paper IDs and subjects into zero-based indices, which is often necessary for machine learning tasks, particularly when dealing with categorical data.\n",
        "- **class_values:** This line creates a sorted list of unique subjects.\n",
        "- **class_idx:** This dictionary comprehension maps each unique subject to a unique zero-based index. For example, if subjects are [\"Case_Based\", \"Genetic_Algorithms\", ...], \"Case_Based\" might be assigned index 0, \"Genetic_Algorithms\" might be assigned index 1, and so on.\n",
        "- **paper_idx:** This dictionary comprehension maps each unique paper ID to a unique zero-based index. This helps in efficiently referencing papers by indices rather than their actual IDs, which can be large and non-sequential.\n",
        "- **The paper_id in the papers DataFrame** and the **source and target columns in the citations DataFrame** are converted to their corresponding indices using the **paper_idx** dictionary.\n",
        "- **Mapping Subjects to Indices:** Finally, it converts the subject column in the papers DataFrame to its corresponding index using the **class_idx** dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYlH7d5HoYT"
      },
      "source": [
        "Now visualize the citation graph. Each node in the graph represents a paper, and the color of the node corresponds to its subject. Note that only a sample of the papers in the dataset is shown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv-p005uHs8r"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "colors = papers[\"subject\"].tolist()\n",
        "cora_graph = nx.from_pandas_edgelist(citations.sample(n=1500))\n",
        "subjects = list(papers[papers[\"paper_id\"].isin(list(cora_graph.nodes))][\"subject\"])\n",
        "nx.draw_spring(cora_graph, node_size=15, node_color=subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuWOS38wOztq"
      },
      "source": [
        "**Explanation of the above graph achieved.**\n",
        "- <u>Graph Layout:</u>\n",
        "  - Spring Layout **(nx.draw_spring):**\n",
        "This layout positions nodes using [Fruchterman-Reingold force-directed algorithm](https://cdn.exec.talentsprint.com/static/cds/content/Fruchterman-Reingold ).\n",
        "Nodes repel each other like physical particles with repulsive forces, and edges act like springs that pull connected nodes together.\n",
        "The result is a balanced visual representation where highly connected nodes (or clusters) are closer together, and less connected nodes are more spread out.\n",
        "- <u>Interpretation of the Graph:</u>\n",
        " - **Central Cluster:** The dense central cluster indicates a group of papers that frequently cite each other. This could represent a well-connected subfield or a core area of research within the larger field.\n",
        " - **Peripheral Nodes:** Nodes on the periphery represent papers that are less connected. These might be more specialized works or newer papers that haven't been cited much yet.\n",
        " - **Node Colors:** Different colors show how papers from different subjects are distributed within the network.\n",
        "If there are noticeable clusters of the same color, it indicates that papers within the same subject tend to cite each other more frequently.\n",
        "Mixed colors within clusters suggest interdisciplinary connections.\n",
        "- <u>Understanding the Sample Size:</u>\n",
        " - **Sample of Papers:** The graph shows only a sample of 1500 papers out of the total dataset. This is done to make the visualization clearer and more manageable.\n",
        "Despite being a sample, the visual can still provide valuable insights into the citation structure and subject distribution.\n",
        "- <u>Practical Uses:</u>\n",
        " - **Identifying central papers and major clusters** can help in understanding key papers and research trends within a field.\n",
        " - **Visualizing how different subjects connect** can help in identifying interdisciplinary research opportunities.\n",
        " - The above graph is useful for performing further network analysis, such as identifying influential papers (nodes with high centrality)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1_wMe9kH3E4"
      },
      "source": [
        "## Split the dataset into stratified train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgOOYTReH7Xk"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = [], []\n",
        "\n",
        "for _, group_data in papers.groupby(\"subject\"):\n",
        "    # Select around 50% of the dataset for training.\n",
        "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
        "    train_data.append(group_data[random_selection])\n",
        "    test_data.append(group_data[~random_selection])\n",
        "\n",
        "train_data = pd.concat(train_data).sample(frac=1)\n",
        "test_data = pd.concat(test_data).sample(frac=1)\n",
        "\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmsAE_LAIEKT"
      },
      "source": [
        "## Implement Train and Evaluate Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2UbfeiTquMO"
      },
      "source": [
        "Setting the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D62_hTL8IFXp"
      },
      "outputs": [],
      "source": [
        "hidden_units = [32, 32]\n",
        "learning_rate = 0.01\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 300\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na2bb2soIJqg"
      },
      "source": [
        "The following function **run_experiment()** compiles and trains an input model using the given training data.\n",
        "\n",
        "This function, **run_experiment()**, trains a given model on training data with the following steps:\n",
        "- **Model Compilation:** The model is compiled with the Adam optimizer, sparse categorical cross-entropy loss, and accuracy metrics.\n",
        "- **Early Stopping:** In this function, an early stopping callback is created to monitor validation accuracy **(val_acc)**. If it doesn't improve for 50 epochs, then training stops, and the best weights are restored.\n",
        "- **Model Training:** The model is trained using the training data **(x_train, y_train)** for a specified number of epochs **(num_epochs)** and batch size **(batch_size)**. The data is split into training and validation sets (85% training, 15% validation). The early stopping callback is used during training.\n",
        "- **Return History:** The training history, which includes loss and accuracy metrics for each epoch, is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0foHEyhINrQ"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model, x_train, y_train):\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.15,\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hum87Fa3IRWP"
      },
      "source": [
        "The following function '**display_learning_curves()**' displays the loss and accuracy curves of the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NLGYSJYIUgM"
      },
      "outputs": [],
      "source": [
        "def display_learning_curves(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(history.history[\"loss\"])\n",
        "    ax1.plot(history.history[\"val_loss\"])\n",
        "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax2.plot(history.history[\"acc\"])\n",
        "    ax2.plot(history.history[\"val_acc\"])\n",
        "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C29tiYqYIlD3"
      },
      "source": [
        "#Implement Feedforward Network (FFN) Module\n",
        "The following code module represents the function 'create_ffn()' which will be used in the baseline and the GNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA_cgDPdAnk2"
      },
      "outputs": [],
      "source": [
        "def create_ffn(hidden_units, dropout_rate, name=None):\n",
        "    # Initialize an empty list to store the layers of the feedforward network\n",
        "    fnn_layers = []\n",
        "\n",
        "    # Iterate over the list of hidden units to create layers\n",
        "    for units in hidden_units:\n",
        "        # Add a Batch Normalization layer to stabilize and accelerate training\n",
        "        fnn_layers.append(layers.BatchNormalization())\n",
        "        # Add a Dropout layer for regularization to prevent overfitting\n",
        "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
        "        # Add a Dense layer with the specified number of units and Gaussian Error Linear Units (GELU) activation\n",
        "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    # Create and return a Sequential model consisting of the defined layers\n",
        "    return keras.Sequential(fnn_layers, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M26MkZlzIu82"
      },
      "source": [
        "#Build a Baseline Neural Network Model\n",
        "\n",
        "This baseline model provides a straightforward way to evaluate the performance of a simple neural network on the dataset before moving on to more complex models such as Graph Neural Networks (GNNs).\n",
        "##Prepare the data for the baseline model\n",
        "- To build a baseline model, the data is first needed to be prepared. This involves selecting features and splitting the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijIAuicwIz70"
      },
      "outputs": [],
      "source": [
        "feature_names = list(set(papers.columns) - {\"paper_id\", \"subject\"})\n",
        "num_features = len(feature_names)\n",
        "num_classes = len(class_idx)\n",
        "\n",
        "# Create train and test features as a numpy array.\n",
        "x_train = train_data[feature_names].to_numpy()\n",
        "x_test = test_data[feature_names].to_numpy()\n",
        "# Create train and test targets as a numpy array.\n",
        "y_train = train_data[\"subject\"]\n",
        "y_test = test_data[\"subject\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu1oKuGGI4JJ"
      },
      "source": [
        "##Implement a baseline classifier\n",
        "In this section, the baseline classifier is a neural network that uses Fully Connected (FFN) blocks to classify papers into different subjects based on their features.\n",
        "- **Benchmarking:** Baseline classifier sets up a performance benchmark for evaluating more complex models, such as Graph Neural Networks (GNNs) to be built in the later part of the assignment.\n",
        "- **Debugging and Validation:** Baseline classifier is implemented to ensure that the data pipeline, feature extraction, and initial model setup are correct.\n",
        "- **Model Comparison:** Baseline classifier provides a comparison point to quantify the improvements made by more advanced models.\n",
        "\n",
        "**FFN (Feedforward Neural Network):** The FFN blocks consist of layers that transform the input data into higher-level representations. By stacking multiple FFN blocks, the model can capture complex patterns and relationships in the data. In this baseline model, adding 5 FFN blocks increases the model's capacity to learn from the data.\n",
        "- **Five FFN blocks with skip connections are added:** By adding five FFN blocks with skip connections, the baseline model is designed to have a similar number of parameters as the GNN models to be built later. This ensures a fair comparison in terms of model complexity and capacity. Having a comparable number of parameters helps in determining whether the GNN architecture provides a genuine performance improvement over the baseline model.\n",
        "- **Skip connections**, also known as residual connections, are used to address the problem of vanishing gradients and to help with training deep networks. They allow the gradient to flow through the network more easily and enable the model to learn more effectively.\n",
        "- Skip connections also help in preventing the degradation of training accuracy as the network depth increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8NQELSQJBkq"
      },
      "outputs": [],
      "source": [
        "def create_baseline_model(hidden_units, num_classes, dropout_rate=0.2):\n",
        "    inputs = layers.Input(shape=(num_features,), name=\"input_features\")\n",
        "    x = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block1\")(inputs)\n",
        "    for block_idx in range(4):\n",
        "        # Create an FFN block.\n",
        "        x1 = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block{block_idx + 2}\")(x)\n",
        "        # Add skip connection.\n",
        "        x = layers.Add(name=f\"skip_connection{block_idx + 2}\")([x, x1])\n",
        "    # Compute logits.\n",
        "    logits = layers.Dense(num_classes, name=\"logits\")(x)\n",
        "    # Create the model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits, name=\"baseline\")\n",
        "\n",
        "\n",
        "baseline_model = create_baseline_model(hidden_units, num_classes, dropout_rate)\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOasr84aSyw6"
      },
      "source": [
        "In the above code cell,\n",
        "- the model starts with an input layer and the first FFN block.\n",
        "- four additional FFN blocks are added with skip connections to create a deep network.\n",
        "- the final layer computes the logits for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqs6pWSCJGuH"
      },
      "source": [
        "##Train the baseline classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziuPw1TQJMp7"
      },
      "outputs": [],
      "source": [
        "history = run_experiment(baseline_model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dGnOBfPUxh_"
      },
      "source": [
        "In the above training logs, both training and validation losses decrease initially, which indicates the model is learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43LA1dYlJRAv"
      },
      "source": [
        "Plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nRrT8eNJXEM"
      },
      "outputs": [],
      "source": [
        "display_learning_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev5asHUJVCqV"
      },
      "source": [
        "**Epochs vs Loss plot:**\n",
        "- both the training and testing loss decrease over time (or no. of epochs) as the model learns from the training data. This indicates that the model is fitting well the training data.\n",
        "- as both training loss and testing loss continue to decrease, it indicates there is no overfitting.\n",
        "\n",
        "**Epochs vs Accuracy plot:**\n",
        "- both the training and testing accuracy increase over time (or no. of epochs) as the model learns from the training data. This indicates that the model is learning well.\n",
        "- as both the accuracies (training & testing) increase initially and then get stabilized, hence it indicates there is no overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTEhfPLzJagt"
      },
      "source": [
        "Now evaluate the baseline model on the test data split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gmQapzQJehH"
      },
      "outputs": [],
      "source": [
        "_, test_accuracy = baseline_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnF6V78kJh1-"
      },
      "source": [
        "##Examine the baseline model predictions\n",
        "\n",
        "This task ensures that the model behaves as expected on unseen data.\n",
        "It also helps in understanding whether the model has learned meaningful patterns from the training data.\n",
        "\n",
        "By evaluating predictions, the performance metrics (e.g., accuracy, precision, recall) can be measured to understand the model's effectiveness.\n",
        "\n",
        "Create new data instances by randomly generating binary word vectors with respect to the word presence probabilities.\n",
        "- creating new data instances by randomly generating binary word vectors based on word presence probabilities serves several purposes:\n",
        "    1. simulates new, unseen data that follows the same distribution as the training data.\n",
        "    2. helps in evaluating how well the model generalizes to new inputs.\n",
        "    3. identifies how the model handles outliers or unusual combinations of features.\n",
        "    4. tests the model's robustness by feeding it random instances.\n",
        "    5. helps in understanding the impact of different features on model predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wpWiXO7SOwB"
      },
      "source": [
        "**In the following code cell**, the **generate_random_instances()** function is designed to create new data instances by randomly generating binary word vectors based on the word presence probabilities observed in the training data.\n",
        "- This function takes one parameter **num_instances** which is the number of new data instances to be generated.\n",
        "- **Calculates Token Probability:**\n",
        "  - **token_probability = x_train.mean(axis=0):** It calculates the mean presence probability of each word token across all training instances. This provides an estimate of the likelihood of each word being present in a randomly generated instance.\n",
        "- **Generates Random Instances:**\n",
        "  - **instances = [ ]:** It initializes an empty list to store the generated instances.\n",
        "  - **for _ in range(num_instances):** This for loop iterates **num_instances** times to generate the specified number of new instances.\n",
        "  - **probabilities = np.random.uniform(size=len(token_probability)):** This generates random probabilities between 0 and 1 for each word token, corresponding to whether the word is present in the instance.\n",
        "  - **instance = (probabilities <= token_probability).astype(int):** It converts the random probabilities to binary values by comparing them with the token probabilities. If the random probability is less than or equal to the token probability, the word is considered present (1), otherwise absent (0).\n",
        "  - **instances.append(instance):** This adds the generated instance to the list of instances.\n",
        "- **Returns Generated Instances:**\n",
        "  - Finally converts the list of generated instances to a NumPy array and returns it.\n",
        "\n",
        "So, effectively the **generate_random_instances()** function allows for the creation of synthetic data instances that mimic the word presence patterns observed in the training data. This helps in evaluating the baseline model's performance and understanding its behavior when faced with new, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfNAe0t7Jm6e"
      },
      "outputs": [],
      "source": [
        "def generate_random_instances(num_instances):\n",
        "    token_probability = x_train.mean(axis=0)\n",
        "    instances = []\n",
        "    for _ in range(num_instances):\n",
        "        probabilities = np.random.uniform(size=len(token_probability))\n",
        "        instance = (probabilities <= token_probability).astype(int)\n",
        "        instances.append(instance)\n",
        "\n",
        "    return np.array(instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaSIdjAbU__A"
      },
      "source": [
        "**In the following code cell**, the **display_class_probabilities()** function is designed to present the predicted probabilities of each class for each instance in a human-readable format.\n",
        "- **Iterating Over Probabilities:**\n",
        "  - **for instance_idx, probs in enumerate(probabilities):** This for loop iterates over the probabilities of each class for each instance.\n",
        "  - **instance_idx** keeps track of the index of the instance in the probabilities array, starting from 0.\n",
        "  - **probs** represents the predicted probabilities for each class for the current instance.\n",
        "- **Printing Instance Index:**\n",
        "  - Prints the index of the current instance along with a label indicating it as an instance.\n",
        "- **Printing Class Probabilities:**\n",
        "  - **for class_idx, prob in enumerate(probs):** This for loop iterates over the predicted probabilities for each class in the current instance.\n",
        "  - **class_idx** represents the index of the class, and **prob** represents the probability of the class.\n",
        "  - **class_values[class_idx]** retrieves the class label associated with the current class index.\n",
        "  - **round(prob * 100, 2)** calculates the probability percentage rounded to two decimal places.\n",
        "  - **Finally it prints** the class label and its corresponding probability in percentage format.\n",
        "\n",
        "So effectively, the following function **display_class_probabilities()** allows for easy interpretation of model predictions by displaying the predicted probabilities of each class for each instance.\n",
        "\n",
        "It also helps users to understand which classes are more likely for each instance, aiding in decision-making and analysis of baseline model performance.\n",
        "\n",
        "By presenting the information in a structured and readable manner, it facilitates the evaluation and comparison of different instances and their associated class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STlDA9UQUdNJ"
      },
      "outputs": [],
      "source": [
        "def display_class_probabilities(probabilities):\n",
        "    for instance_idx, probs in enumerate(probabilities):\n",
        "        print(f\"Instance {instance_idx + 1}:\")\n",
        "        for class_idx, prob in enumerate(probs):\n",
        "            print(f\"- {class_values[class_idx]}: {round(prob * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxPGb7cJrZI"
      },
      "source": [
        "Now show the baseline model predictions given these randomly generated instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwL1olGjJubt"
      },
      "outputs": [],
      "source": [
        "new_instances = generate_random_instances(num_classes)\n",
        "logits = baseline_model.predict(new_instances)\n",
        "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
        "display_class_probabilities(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsQKF-etiaKc"
      },
      "source": [
        "The above results show 7 instances with the distribution of the class probabilities corresponding to each subject category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liD3nOvHJzqE"
      },
      "source": [
        "#Build a Graph Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrIueWE8rck"
      },
      "source": [
        "##Prepare the data for the graph model\n",
        "\n",
        "Preparing and loading graph data for training is a critical and challenging task in GNN models, typically addressed by specialized libraries. Here, a simple approach is demonstrated and this approach is suitable for datasets consisting of a single graph that fits entirely in memory.\n",
        "\n",
        "The graph data is encapsulated in the graph_info tuple, which includes:\n",
        "\n",
        "1. **edges**: A [num_edges, num_edges] NumPy array representing a sparse adjacency matrix of links (citations) between nodes (papers).\n",
        "2. **edge_weights** (optional): A [num_edges] NumPy array of edge weights, all set to one in this example, implying equal influence of all edges.\n",
        "3. **node_features**: A [num_nodes, num_features] NumPy array where each row represents a node (paper) with a binary feature vector indicating word presence.\n",
        "\n",
        "For instance, the node_features array has 2708 rows (papers) and 1433 columns (binary word features). The graph_info tuple thus encapsulates the node features, edges, and edge weights, ready for input to a GNN model. This preparation enables the GNN to learn relationships between papers via edges and classify papers using their feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8lnIDOpKaxQ"
      },
      "outputs": [],
      "source": [
        "# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
        "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
        "# Create an edge weights array of ones.\n",
        "edge_weights = tf.ones(shape=edges.shape[1])\n",
        "# Create a node features array of shape [num_nodes, num_features].\n",
        "node_features = tf.cast(\n",
        "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
        ")\n",
        "# Create graph info tuple with node_features, edges, and edge_weights.\n",
        "graph_info = (node_features, edges, edge_weights)\n",
        "\n",
        "print(\"Edges shape:\", edges.shape)\n",
        "print(\"Nodes shape:\", node_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0edle1jKhRGY"
      },
      "source": [
        "The results above indicate the successful preparation of the data for building a Graph Neural Network (GNN) model using the Cora dataset.\n",
        "- **Edges shape:** The array has 2 rows and 5429 columns. Each column represents an edge (a citation) in the graph, with the first row containing the source nodes (papers) and the second row containing the target nodes (papers). There are 5429 citations (edges) in total.\n",
        "- **Nodes shape:** The array has 2708 rows and 1433 columns. Each row corresponds to a paper (node) and each column corresponds to a binary feature indicating the presence or absence of a word. There are 2708 papers (nodes) in total, and each paper is represented by a 1433-dimensional feature vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBlLyx6HKcGR"
      },
      "source": [
        "##Implement a graph convolution layer\n",
        "In the following section, a graph convolution module is implemented as a Keras Layer.\n",
        "\n",
        "The technique implemented uses ideas from Graph Convolutional Networks, GraphSage, Graph Isomorphism Network, Simple Graph Networks, and Gated Graph Sequence Neural Networks.\n",
        "\n",
        "Two other key techniques (Graph Attention Networks and Message Passing Neural Networks) are not covered in implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEIp2pgBYUqs"
      },
      "source": [
        "**The following code cell** implements the function **create_gru()**.\n",
        "\n",
        "The function **create_gru()** creates a Keras model consisting of multiple stacked GRU (Gated Recurrent Unit) layers. This model can be used here as the tasks involve sequential data.\n",
        "- **hidden_units:** This is a list of integers where each integer specifies the number of units (neurons) in a GRU layer.\n",
        "- **dropout_rate:** This is a float representing the dropout rate, which is used to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.\n",
        "\n",
        "**GRU Layer Parameters:**\n",
        "- **units:** Number of units (neurons) in the GRU layer.\n",
        "- **activation:** Activation function for the output **(tanh)**.\n",
        "- **recurrent_activation:** Activation function for the recurrent step **(sigmoid)**.\n",
        "- **return_sequences:** If True, the GRU layer returns the full sequence output for each input sequence (required when stacking multiple GRU layers). Here, it is set as True.\n",
        "- **dropout:** Dropout rate for the input units.\n",
        "- **return_state:** If False, the GRU layer returns only the output. If True, it also returns the last state. Here it is set as False.\n",
        "- **recurrent_dropout:** Dropout rate for the recurrent state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7hig_uSYJht"
      },
      "outputs": [],
      "source": [
        "def create_gru(hidden_units, dropout_rate):\n",
        "    inputs = keras.layers.Input(shape=(2, hidden_units[0]))\n",
        "    x = inputs\n",
        "    for units in hidden_units:\n",
        "      x = layers.GRU(\n",
        "          units=units,\n",
        "          activation=\"tanh\",\n",
        "          recurrent_activation=\"sigmoid\",\n",
        "          return_sequences=True,\n",
        "          dropout=dropout_rate,\n",
        "          return_state=False,\n",
        "          recurrent_dropout=dropout_rate,\n",
        "      )(x)\n",
        "    return keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NVh66jZhTR"
      },
      "source": [
        "**In the following code cell**, the **GraphConvLayer** class is implemented.\n",
        "\n",
        "This **GraphConvLayer** performs the following steps:\n",
        "\n",
        "1. **Prepare**: The input node representations are processed using a FFN to produce a message. The processing can be simplified  by only applying linear transformation to the representations. It calls another method **ffn_prepare** within the same class to process the node representations. The result is stored in the **messages** variable.\n",
        "\n",
        "This is the processed **messages** tensor. If weights were provided, they are applied to the messages before returning the result.\n",
        "2. **Aggregate**: The messages of the neighbours of each node are aggregated with respect to the **edge_weights** using a permutation invariant pooling operation, such as **sum, mean, and max,** to prepare a single aggregated message for each node. For example, the **tf.math.unsorted_segment_sum** APIs are used to aggregate neighbour messages.\n",
        "It **Computes** the number of nodes in the graph based on the specified aggregation type:\n",
        "- **If \"sum\"**, aggregates neighbour messages using tf.math.unsorted_segment_sum.\n",
        "- **If \"mean\"**, aggregates using tf.math.unsorted_segment_mean.\n",
        "- **If \"max\"**, aggregates using tf.math.unsorted_segment_max.\n",
        "- **Otherwise**, raises a ValueError for invalid aggregation types.\n",
        "- **Output:**\n",
        "  - **Finally returns** the aggregated message tensor.\n",
        "\n",
        "3. **Update**: The **node_repesentations** and **aggregated_messages**  both of shape [num_nodes, representation_dim] are combined and processed to produce the new state of the node representations **(node embeddings).** If **combination_type** is **gru**, the **node_repesentations** and **aggregated_messages** are stacked to create a sequence, then processed by a GRU layer. Otherwise, the **node_repesentations** and **aggregated_messages** are added or concatenated, then processed using the FFN.\n",
        "It **Applies the processing function (update_fn)** to the combined representations. **When using a GRU (Gated Recurrent Unit)** for combining node representations and aggregated messages, the GRU produces a sequence of outputs.\n",
        "Then, the **tf.unstack(node_embeddings, axis=1)** separates this sequence along the specified axis (axis=1), creating a list of tensors.\n",
        "           \n",
        "  The **Operation: tf.nn.l2_normalize(node_embeddings, axis=-1)** normalizes each embedding vector along the last axis (axis=-1), dividing each vector by its **L2 norm** (the square root of the sum of the squared vector values). **Finally it returns** the updated node embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9nW15SmK5p9"
      },
      "outputs": [],
      "source": [
        "class GraphConvLayer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_units,\n",
        "        dropout_rate=0.2,\n",
        "        aggregation_type=\"mean\",\n",
        "        combination_type=\"concat\",\n",
        "        normalize=False,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.aggregation_type = aggregation_type\n",
        "        self.combination_type = combination_type\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
        "        if self.combination_type == \"gru\":\n",
        "            self.update_fn = create_gru(hidden_units, dropout_rate)\n",
        "        else:\n",
        "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
        "\n",
        "    def prepare(self, node_repesentations, weights=None):\n",
        "        # node_repesentations shape is [num_edges, embedding_dim].\n",
        "        messages = self.ffn_prepare(node_repesentations)\n",
        "        if weights is not None:\n",
        "            messages = messages * tf.expand_dims(weights, -1)\n",
        "        return messages\n",
        "\n",
        "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
        "        # node_indices shape is [num_edges].\n",
        "        # neighbour_messages shape: [num_edges, representation_dim].\n",
        "        # node_repesentations shape is [num_nodes, representation_dim]\n",
        "        num_nodes = node_repesentations.shape[0]\n",
        "        if self.aggregation_type == \"sum\":\n",
        "            aggregated_message = tf.math.unsorted_segment_sum(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"mean\":\n",
        "            aggregated_message = tf.math.unsorted_segment_mean(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"max\":\n",
        "            aggregated_message = tf.math.unsorted_segment_max(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
        "\n",
        "        return aggregated_message\n",
        "\n",
        "    def update(self, node_repesentations, aggregated_messages):\n",
        "        # node_repesentations shape is [num_nodes, representation_dim].\n",
        "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
        "        if self.combination_type == \"gru\":\n",
        "            # Create a sequence of two elements for the GRU layer.\n",
        "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"concat\":\n",
        "            # Concatenate the node_repesentations and aggregated_messages.\n",
        "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"add\":\n",
        "            # Add node_repesentations and aggregated_messages.\n",
        "            h = node_repesentations + aggregated_messages\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
        "\n",
        "        # Apply the processing function.\n",
        "        node_embeddings = self.update_fn(h)\n",
        "        if self.combination_type == \"gru\":\n",
        "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
        "\n",
        "        if self.normalize:\n",
        "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
        "        return node_embeddings\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Process the inputs to produce the node_embeddings.\n",
        "\n",
        "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
        "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
        "        \"\"\"\n",
        "\n",
        "        node_repesentations, edges, edge_weights = inputs\n",
        "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
        "        node_indices, neighbour_indices = edges[0], edges[1]\n",
        "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
        "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
        "\n",
        "        # Prepare the messages of the neighbours.\n",
        "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
        "        # Aggregate the neighbour messages.\n",
        "        aggregated_messages = self.aggregate(\n",
        "            node_indices, neighbour_messages, node_repesentations\n",
        "        )\n",
        "        # Update the node embedding with the neighbour messages.\n",
        "        return self.update(node_repesentations, aggregated_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzkrz1fxLSyn"
      },
      "source": [
        "##Implement a graph neural network node classifier\n",
        "\n",
        "The GNN classification model follows the Design Space for Graph Neural Networks approach, as follows:\n",
        "\n",
        "1. Apply preprocessing using FFN to the node features to generate initial node representations.\n",
        "2. Apply one or more graph convolutional layer, with skip connections, to the node representation to produce node embeddings.\n",
        "3. Apply post-processing using FFN to the node embeddings to generate the final node embeddings.\n",
        "4. Feed the node embeddings in a Softmax layer to predict the node class.\n",
        "Each graph convolutional layer added captures information from a further level of neighbours. However, adding many graph convolutional layer can cause oversmoothing, where the model produces similar embeddings for all the nodes.\n",
        "\n",
        "Note that the graph_info passed to the constructor of the Keras model, and used as a property of the Keras model object, rather than input data for training or prediction. The model will accept a **batch** of **node_indices**, which are used to lookup the node features and neighbours from the **graph_info**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpzUIAtXLuXs"
      },
      "outputs": [],
      "source": [
        "class GNNNodeClassifier(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        graph_info,\n",
        "        num_classes,\n",
        "        hidden_units,\n",
        "        aggregation_type=\"sum\",\n",
        "        combination_type=\"concat\",\n",
        "        dropout_rate=0.2,\n",
        "        normalize=True,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
        "        node_features, edges, edge_weights = graph_info\n",
        "        self.node_features = node_features\n",
        "        self.edges = edges\n",
        "        self.edge_weights = edge_weights\n",
        "        # Set edge_weights to ones if not provided.\n",
        "        if self.edge_weights is None:\n",
        "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
        "        # Scale edge_weights to sum to 1.\n",
        "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
        "\n",
        "        # Create a process layer.\n",
        "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
        "        # Create the first GraphConv layer.\n",
        "        self.conv1 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv1\",\n",
        "        )\n",
        "        # Create the second GraphConv layer.\n",
        "        self.conv2 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv2\",\n",
        "        )\n",
        "        # Create a postprocess layer.\n",
        "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
        "        # Create a compute logits layer.\n",
        "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
        "\n",
        "    def call(self, input_node_indices):\n",
        "        # Preprocess the node_features to produce node representations.\n",
        "        x = self.preprocess(self.node_features)\n",
        "        # Apply the first graph conv layer.\n",
        "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x1 + x\n",
        "        # Apply the second graph conv layer.\n",
        "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x2 + x\n",
        "        # Postprocess node embedding.\n",
        "        x = self.postprocess(x)\n",
        "        # Fetch node embeddings for the input node_indices.\n",
        "        node_embeddings = tf.gather(x, input_node_indices)\n",
        "        # Compute logits\n",
        "        return self.compute_logits(node_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvPhp1G5Lvh1"
      },
      "source": [
        "Test instantiating and calling the GNN model. Notice that if N node indices are provided, the output will be a tensor of shape **[N, num_classes]**, regardless of the **size of the graph**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0GlZug2Ly25"
      },
      "outputs": [],
      "source": [
        "gnn_model = GNNNodeClassifier(\n",
        "    graph_info=graph_info,\n",
        "    num_classes=num_classes,\n",
        "    hidden_units=hidden_units,\n",
        "    dropout_rate=dropout_rate,\n",
        "    name=\"gnn_model\",\n",
        ")\n",
        "\n",
        "# Convert the list of node indices to a Tensorflow tensor.\n",
        "input_node_indices = tf.constant([1, 10, 100])\n",
        "\n",
        "# Call the model with the input_node_indices tensor.\n",
        "print(\"GNN output shape:\", gnn_model(input_node_indices))\n",
        "\n",
        "gnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHhEATZvkMp"
      },
      "source": [
        "The model architecture consists the following sequential layers:\n",
        "\n",
        "**Input Node Features --> Preprocess Layer --> Graph Conv Layer 1 --> Graph Conv Layer 2 --> Postprocess Layer --> Dense Layer (Logits) --> Class Probabilities**\n",
        "\n",
        "**Layers and Their Roles in the above GNN model summary:**\n",
        "1. Preprocess (Sequential):\n",
        "  - Output Shape: (2708, 32)\n",
        "  - Param #: 52804\n",
        "  - This layer processes the input node features, transforming them into a 32-dimensional space. It consists of a sequence of operations, typically including linear transformations and possibly some non-linear activations.\n",
        "2. GraphConvLayer (graph_conv1):\n",
        "  - Output Shape: Multiple\n",
        "  - Param #: 5888\n",
        "  - This is the first graph convolution layer. It aggregates information from neighboring nodes to update the node representations.\n",
        "3. GraphConvLayer (graph_conv2):\n",
        "  - Output Shape: Multiple\n",
        "  - Param #: 5888\n",
        "  - This is the second graph convolution layer, further refining the node representations by aggregating information from the neighbors.\n",
        "4. Postprocess (Sequential):\n",
        "  - Output Shape: (2708, 32)\n",
        "  - Param #: 2368\n",
        "  - This layer further processes the node representations after the graph convolution layers. It typically involves additional transformations to prepare the data for the final classification layer.\n",
        "5. Logits (Dense):\n",
        "  - Output Shape: Multiple\n",
        "  - Param #: 231\n",
        "  - This is the number of parameters in the dense layer, which includes the weights and biases. For a dense layer with 32 input units and 7 output units (one for each class), you would have $32  7 = 224$ weights plus $7$ biases, resulting in $231$ parameters.\n",
        "  - This is the final dense layer that produces the logits for each of the $7$ classes. The output shape corresponds to the number of classes (i.e., $7$), and the logits are the raw scores before applying the softmax function to get the class probabilities.\n",
        "  - The dense layer's output shape \"multiple\" indicates that it can handle different batch sizes or number of nodes during prediction. As here the gnn_model predicts for specific nodes i.e., $[1, 10, 100]$, the model processes the features for these $3$ nodes and outputs logits for each node across all 7 classes. **Hence, the output shape becomes $(3, 7)$**.\n",
        "\n",
        "**Parameters in the above GNN Model:**\n",
        "  - Total params: 67179\n",
        "\n",
        "    - The total number of parameters in the model, including both trainable and non-trainable parameters.\n",
        "  - Trainable params: 63481\n",
        "    - The number of parameters that will be updated during training.\n",
        "  - Non-trainable params: 3698\n",
        "    - The number of parameters that are not updated during training. These could be static weights or hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxCguy_nL2Zw"
      },
      "source": [
        "##Train the GNN model\n",
        "Note that we use the standard supervised cross-entropy loss to train the model. However, another self-supervised loss term can be added for the generated node embeddings that makes sure that neighbouring nodes in graph have similar representations, while faraway nodes have dissimilar representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMROAjGiL77u"
      },
      "outputs": [],
      "source": [
        "x_train = train_data.paper_id.to_numpy()\n",
        "history = run_experiment(gnn_model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD-PCnFNL_Dg"
      },
      "source": [
        "Plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsEQYiNBMEY4"
      },
      "outputs": [],
      "source": [
        "display_learning_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcdWvsXN7QLN"
      },
      "source": [
        "From the above plots, the following interpretations can be drawn.\n",
        "\n",
        "**Interpretation for Epoch vs Loss plot of the GNN Model:**\n",
        "\n",
        "- Initial High Loss: At the start of training, both the training and validation losses are high. This indicates that the GNN model is not yet well-fitted to the data.\n",
        "- Decreasing Training Loss: As training (i.e., the No. of Epochs) progresses, the training loss decreases significantly, indicating that the model is learning from the training data and improving its fit. The final training loss is low, showing effective learning.\n",
        "- Decreasing Validation Loss: The validation loss also decreases initially, suggesting that the model is generalizing well to unseen data. Periodic spikes in validation loss can occur due to the model encountering particularly difficult batches of validation data.\n",
        "- Flattening Out: After many epochs (i.e., after 160 Epochs), both the training and validation losses tend to plateau, indicating that the model has reached a state of near-optimal performance.\n",
        "- Convergence: By the end of training (i.e., around epoch 300), the training loss is very low, and the validation loss has decreased considerably from its initial value.\n",
        "\n",
        "**Interpretation for Epoch vs Accuracy plot of the GNN Model:**\n",
        "- Initial Low Accuracy: At the start of training, both the training and validation accuracies are low, indicating that the model is not yet making good predictions.\n",
        "- Increasing Training Accuracy: As training (i.e., No. of Epochs) progresses, the training accuracy increases, showing that the model is learning and making better predictions on the training data.\n",
        "- Increasing Validation Accuracy: The validation accuracy also increases, suggesting that the model is generalizing well to unseen data.\n",
        "- Plateauing: After many epochs (i.e., afyer 160 Epochs), both the training and validation accuracies tend to plateau, indicating that further training is providing diminishing returns.\n",
        "- High Final Accuracy: By the end of training (i.e., around epoch 300), the training accuracy is very high, often approaching 90-100%.\n",
        "- Validation Accuracy: The validation accuracy is also significantly improved from the start, though it may be slightly lower than the training accuracy due to some amount of overfitting or noise in the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT0vYDDMMHHG"
      },
      "source": [
        "Now evaluate the GNN model on the test data split. The results may vary depending on the training sample, however the GNN model always outperforms the baseline model in terms of the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHXnEXFQMJ_e"
      },
      "outputs": [],
      "source": [
        "x_test = test_data.paper_id.to_numpy()\n",
        "_, test_accuracy = gnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP8M3TSI_CI3"
      },
      "source": [
        "As per the above result, the **Test accuracy of $80.7$%** indicates that the GNN model correctly classifies approximately $80.7%$ of the papers in the test dataset.\n",
        "\n",
        "The GNN model outperforms the baseline model. This is because GNNs are designed to make use of the structural information inherent in graph data, such as in this case, the citation network in the Cora dataset.\n",
        "\n",
        "By incorporating information from neighboring nodes (i.e., citations), the GNN can capture more complex patterns and relationships than a baseline model, which may only use node features without considering the graph structure.\n",
        "\n",
        "The GNN model maintains superior accuracy compared to the baseline. The test accuracy reflects the model's ability to generalize well to new data, indicating that it is not overfitted to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V1dYs65MNG_"
      },
      "source": [
        "##Examine the GNN model predictions\n",
        "Now add the new instances as nodes to the **node_features**, and generate links (citations) to existing nodes. The following code cell adds num_nodes to the graph by appending the new_instances to node_features. The steps are described below.\n",
        "\n",
        "1. **Add New Nodes to the Graph:**\n",
        "   - **num_nodes:** This is the original number of nodes in the graph.\n",
        "   - **new_node_features:** This is a new array of node features created by appending the features of the new instances (new_instances) to the original node features (node_features).\n",
        "   - **Significance:** The Node Addition process integrates new nodes (representing new instances) into the existing graph, expanding its structure.\n",
        "2. **Identify New Node Indices:**\n",
        "   - **new_node_indices:** These are the indices for the new nodes, calculated by adding the total number of original nodes to the range of the number of classes.\n",
        "   - **Significance:** This step identifies new node indices which ensures that the newly added nodes are correctly indexed and distinct from the original nodes. It also maintains the integrity of the graph structure and enables accurate reference & manipulation in subsequent steps.\n",
        "3. **Create New Citations (Edges):**\n",
        "   - **new_citations:** It is a list to store the new edges.\n",
        "   - **Loop through** each subject group in the papers DataFrame.\n",
        "   - **For each subject group:**\n",
        "     - **subject_papers:** This is the List of papers belonging to the current subject.\n",
        "     - **selected_paper_indices1:** Randomly select 5 papers from the current subject.\n",
        "     - **selected_paper_indices2:** Randomly select 2 papers from any subject.\n",
        "     - **selected_paper_indices:** Concatenate the selected indices from the current subject and any subject.\n",
        "     - **citing_paper_indx:** This is the index of the new node that cites other papers.\n",
        "     - **Create new edges** from the new node (citing_paper_indx) to the selected papers (cited_paper_idx).\n",
        "   - **Significance:** New Edges creation establishes relationships (citations) between new nodes and existing nodes, reflecting the interconnected nature of the data.\n",
        "4. **Update Edge List:**\n",
        "     - **new_citations:** Convert the list of new citations to a NumPy array and transpose it.\n",
        "     - **new_edges:** Concatenate the original edges with the new citations to create the updated list of edges.\n",
        "     - **Significance:** Tthe final concatenation step updates the graph (Graph update) to include both the new nodes and the new edges, making it ready for further analysis or model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgt1-tocMVC9"
      },
      "outputs": [],
      "source": [
        "# First add the N new_instances as nodes to the graph\n",
        "# by appending the new_instance to node_features.\n",
        "num_nodes = node_features.shape[0]\n",
        "new_node_features = np.concatenate([node_features, new_instances])\n",
        "# Second add the M edges (citations) from each new node to a set\n",
        "# of existing nodes in a particular subject\n",
        "new_node_indices = [i + num_nodes for i in range(num_classes)]\n",
        "new_citations = []\n",
        "for subject_idx, group in papers.groupby(\"subject\"):\n",
        "    subject_papers = list(group.paper_id)\n",
        "    # Select random x papers specific subject.\n",
        "    selected_paper_indices1 = np.random.choice(subject_papers, 5)\n",
        "    # Select random y papers from any subject (where y < x).\n",
        "    selected_paper_indices2 = np.random.choice(list(papers.paper_id), 2)\n",
        "    # Merge the selected paper indices.\n",
        "    selected_paper_indices = np.concatenate(\n",
        "        [selected_paper_indices1, selected_paper_indices2], axis=0\n",
        "    )\n",
        "    # Create edges between a citing paper idx and the selected cited papers.\n",
        "    citing_paper_indx = new_node_indices[subject_idx]\n",
        "    for cited_paper_idx in selected_paper_indices:\n",
        "        new_citations.append([citing_paper_indx, cited_paper_idx])\n",
        "\n",
        "new_citations = np.array(new_citations).T\n",
        "new_edges = np.concatenate([edges, new_citations], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNCXxRqMdIc"
      },
      "source": [
        "Now update the node_features and the edges in the GNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8GTcEyPMgko"
      },
      "outputs": [],
      "source": [
        "print(\"Original node_features shape:\", gnn_model.node_features.shape)\n",
        "print(\"Original edges shape:\", gnn_model.edges.shape)\n",
        "gnn_model.node_features = new_node_features\n",
        "gnn_model.edges = new_edges\n",
        "gnn_model.edge_weights = tf.ones(shape=new_edges.shape[1])\n",
        "print(\"New node_features shape:\", gnn_model.node_features.shape)\n",
        "print(\"New edges shape:\", gnn_model.edges.shape)\n",
        "\n",
        "logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices))\n",
        "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
        "display_class_probabilities(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgmNphzwMjfd"
      },
      "source": [
        "In the above results, notice that the probabilities of the expected subjects (to which several citations are added) are higher compared to the baseline model.\n",
        "\n",
        "- **Original node_features shape:** (2708, 1433): This indicates that the original graph had 2,708 nodes (papers), each represented by a feature vector of size 1,433.\n",
        "- **Original edges shape:** (2, 5429): This shows there were 5,429 edges (citations) in the original graph.\n",
        "- **New node_features shape:** (2715, 1433): After adding the new instances, the graph now has 2,715 nodes.\n",
        "- **New edges shape:** (2, 5478): After adding the new citations, the graph now has 5,478 edges.\n",
        "\n",
        "The above results show 7 instances with the Class Probabilities for New Instances corresponding to each subject category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7_3xcUtNVHU"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFTeoD7QNgyy"
      },
      "outputs": [],
      "source": [
        "# @title Select the correct statement: In the create_baseline_model() function, the skip_connection is used in order to address the problem of { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param[\"\",\"memory leakage and to help with training deep networks.\",\"vanishing gradients and to help with training deep networks.\",\"underfitting and to reduce model complexity.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9byW_mF6O8OR"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzhlzuqcPB1H"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVq-lVhAPHZ8"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLJnyqeBPLWK"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv0Jz8CsPP_5"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qsMDsBXHPUvb"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}